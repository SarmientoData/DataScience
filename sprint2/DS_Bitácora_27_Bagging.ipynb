{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "DS_Bitácora_27_Bagging.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6_iSwBXBJvqk",
        "Mc_mkQLyJvrC",
        "pQNvu9PlJvr7"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nLhAya1JvpA"
      },
      "source": [
        "# Ensambles - Bagging\n",
        "\n",
        "A lo largo del notebook vamos a trabajar con el siguiente dataset:\n",
        "\n",
        "https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
        "\n",
        "El objetivo de este analisis será predecir si lloverá o no al día siguiente.\n",
        "\n",
        "## 1. EDA y Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hef9zJ4JvpD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UmmNd16JvpO"
      },
      "source": [
        "1. Abrir el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCQgrC5OJvpT"
      },
      "source": [
        "data = pd.read_csv(\"DS_Encuentro_27_Weather.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oweAXg_nJvpb"
      },
      "source": [
        "Contamos cuántos valores no-nulos hay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC3UJn1PJvpd"
      },
      "source": [
        "data.count().sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONJ845VYJvpk"
      },
      "source": [
        "2. Tirar las columnas que no nos interesan, entre ellas las que tienen pocos datos (menos de cien mil). Además, tirar 'Location' y 'Date', ya que no nos interesa el lugar ni fecha (al menos en este análisis), y 'RISK_MM', porque es un *leak*. **Para googlear**: ¿qué es un leak?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQh_qzUQJvpm"
      },
      "source": [
        "columnas_descartables = [COMPLETAR]\n",
        "data = data.drop(columns=columnas_descartables)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-NK0lB1Jvpt"
      },
      "source": [
        "3. Tirar todas las filas que tengan valores nulos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFtAJxwdJvpu"
      },
      "source": [
        "data = data.COMPLETAR()\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCaiTdARJvp2"
      },
      "source": [
        "4. Para simplificar el preprocesamiento, también tirar todas las columnas que tengan valores categóricos. ¿Por qué no nos molesta tirar 'RainToday'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac96jl7qJvp4"
      },
      "source": [
        "columnas_descartables = [COMPLETAR]\n",
        "data = data.drop(columns=columnas_descartables)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJEIzdGpJvp9"
      },
      "source": [
        "5. Realizar un countplot para ver cuántos casos hay de lluvia y no-lluvia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYe4U3KkJvp-"
      },
      "source": [
        "sns.COMPLETAR(data.COMPLETAR)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE5WNUyDJvqE"
      },
      "source": [
        "Y hacer el `pairplot` para ver cómo se relacionan las variables. Recuerden que este gráfico puede llevar bastante tiempo. También recuerden que pueden agrandar el gráfico haciendo doble click en él."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCg_9qpqJvqG"
      },
      "source": [
        "sns.pairplot(data.sample(frac = 0.1), hue = 'RainTomorrow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUwv2uCOJvqN"
      },
      "source": [
        "Hay algunas que parecen *correlacionadas*. Tratamos de cuantificarlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQfAuhVIJvqO"
      },
      "source": [
        "corr = data.drop(columns = ['RainTomorrow']).corr(method='spearman') # .corr is used for find corelation\n",
        "plt.figure(figsize=(14,14))\n",
        "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n",
        "           xticklabels= data.drop(columns = ['RainTomorrow']).columns, \n",
        "           yticklabels= data.drop(columns = ['RainTomorrow']).columns,\n",
        "           cmap= 'coolwarm')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teNYT08HJvqU"
      },
      "source": [
        "En base a la correlación, podemos descartar (o no) algunas variables. **Para pensar**, ¿por qué haríamos (o no) esto?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4r03pC2JvqX"
      },
      "source": [
        "data = data.drop(columns=[COMPLETAR, COMPLETAR])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NkB6JNlJvqc"
      },
      "source": [
        "6. Llevar `RainTomorrow` a una variable númerica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHiBV7ApJvqd"
      },
      "source": [
        "data[COMPLETAR] = data[COMPLETAR].map(COMPLETAR)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_iSwBXBJvqk"
      },
      "source": [
        "### Datos de entrenamiento y casos *benchmark*\n",
        "\n",
        "Generamos casos base contra los cuales comparar nuestros resultados.\n",
        "\n",
        "1. Elegir variables de entrenamiento (empezar con dos) y separar las etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YyqzqV-Jvqk"
      },
      "source": [
        "columnas_entrenamiento = [COMPLETAR]\n",
        "X = data[columnas_entrenamiento]\n",
        "y = data.COMPLETAR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8731_NvkJvqp"
      },
      "source": [
        "2. Generar un modelo que diga siempre que NO va a llover y medir su exactitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_-jbcCdJvqq"
      },
      "source": [
        "#Todos Ceros\n",
        "y_pred = np.COMPLETAR(y.shape)\n",
        "accuracy_ceros = metrics.COMPLETAR(y,y_pred)\n",
        "print(accuracy_ceros)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySMoGKeZJvqv"
      },
      "source": [
        "Y generar otro modelo que diga siempre que va a llover y medir su exactitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGWQ7nglJvqw"
      },
      "source": [
        "#Todos Unos\n",
        "y_pred = COMPLETAR\n",
        "accuracy_unos = COMPLETAR\n",
        "print(accuracy_unos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvd3b7d4Jvq0"
      },
      "source": [
        "**3. - Challenge:**  Entrena un árbol de decisión sobre este dataset. Intenta obtener el mejor desempeño que creas posible, optimizando sus hiperparámetros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8gWqes4Jvq1"
      },
      "source": [
        "# COMPLETAR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc_mkQLyJvrC"
      },
      "source": [
        "## 2. Bagging\n",
        "\n",
        "Separamos entre train y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUISoV0dJvrD"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify = y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDdZ0iLBJvrI"
      },
      "source": [
        "Recuerden que el objetivo de bagging es entrenar distintos modelos, donde cada uno vea distintas porciones del set de entrenamiento. Entonces, vamos a entrenar distintos árboles de decisión y mostrarles distintas porciones del set de datos. Lo vamos a hacer en un `for`.\n",
        "\n",
        "1. Crear una lista vacía donde guardaremos los modelos entrenados y elegir cuántos modelos entrenar (Empezar por algún valor entre 5 y 10)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ztTJTETJvrJ"
      },
      "source": [
        "lista_de_modelos = COMPLETAR\n",
        "N_modelos = COMPLETAR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaJ8I7cdJvrN"
      },
      "source": [
        "2. Entrenar cada modelo y guardar cada modelo entrenado en una lista. Para hacer el split, usar la función `train_test_split`. ¿Sobre qué conjunto van a hacer el split?¿Hay que fijar el `random_state`? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_otcyfbJvrO"
      },
      "source": [
        "for i in range(COMPLETAR):\n",
        "    X_train_boostrap, _, y_train_boostrap, _ = train_test_split(COMPLETAR, COMPLETAR, test_size=0.5, stratify = COMPLETAR)\n",
        "    clf = DecisionTreeClassifier(max_depth = None) #Notar que lo dejamos overfitear\n",
        "    clf.fit(COMPLETAR, COMPLETAR)\n",
        "    lista_de_modelos.append(clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpsPf5kFJvrR"
      },
      "source": [
        "3. Evaluar el accuracy de cada modelo usando el conjunto de held_out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsgzTND2JvrS"
      },
      "source": [
        "for idx, modelo in enumerate(COMPLETAR):\n",
        "    y_test_pred = modelo.predict(COMPLETAR)\n",
        "    print('Accuracy Modelo ', idx, ' es ', metrics.COMPLETAR(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdgMthaaJvrW"
      },
      "source": [
        "Parecen estar un poco overfitteados, que era lo que esperábamos.\n",
        "\n",
        "4. Evaluar el accuracy de todo el ensamble usando el conjunto de held_out. Vamos a hacerlo usando un promedio de las probabilidades que devuelven cada árbol. Si la probabilidad promedio es mayor a 0.5, clasificamos como positivo. Para ello:\n",
        "    1. Inicializar un arreglo de probabilidades del tamaño de la cantidad de instancias del conjunto de test en ceros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUEvoz7sJvrX"
      },
      "source": [
        "probs_test_pred = np.COMPLETAR(COMPLETAR.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVesaBT4Jvrb"
      },
      "source": [
        "B. Recorrer la lista de modelos y predecir las probabilidades. Mirar como es el `shape` de ese arreglo predicho. Elegir las probabilidades que correspondan a la clase positiva. Luego, sumarlas al vector que definieron antes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz7Za7DZJvre"
      },
      "source": [
        "for modelo in lista_de_modelos:\n",
        "    probs_test_pred_modelo = modelo.COMPLETAR(X_test)\n",
        "    print(probs_test_pred_modelo.shape)\n",
        "    # Cuando esten seguros de lo que quieran sumar, descomentar la linea de abajo y completar\n",
        "#     probs_test_pred +=probs_test_pred_modelo[COMPLETAR,COMPLETAR]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmv52DebJvri"
      },
      "source": [
        "C. Dividir `probs_test_pred` por la cantidad de modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xuoE2eSJvrj"
      },
      "source": [
        "probs_test_pred = probs_test_pred/COMPLETAR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMKCvgCNJvrn"
      },
      "source": [
        "D. Crear las clases predichas (0s y 1s) a partir de comparar la probabilidad predicha con la probabilidad umbral (0.5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20crY7SYJvro"
      },
      "source": [
        "y_test_pred = probs_test_pred>COMPLETAR\n",
        "y_test_pred = y_test_pred.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A3BvrucJvrr"
      },
      "source": [
        "Y evaluar la exactitud de todo el ensamble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFh0l13tJvrs"
      },
      "source": [
        "print('Accuracy Ensambe ', metrics.accuracy_score(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APxl2HcfJvrw"
      },
      "source": [
        "5. Explorar el `BagginClassfier` de scikit-learn y algunas de sus características. Usarlo para predecir sobre el train y test, y medir su desempeño."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1dcDvhTJvrw"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDFRfqvLJvr0"
      },
      "source": [
        "clf = BaggingClassifier(base_estimator=COMPLETAR, bootstrap = COMPLETAR, bootstrap_features=COMPLETAR, n_estimators = 100, n_jobs = -1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = clf.predict(X_train)\n",
        "y_test_pred = clf.predict(X_test)\n",
        "\n",
        "print(metrics.accuracy_score(y_train, y_train_pred))\n",
        "print(metrics.accuracy_score(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAuLOo02Jvr4"
      },
      "source": [
        "6. Si usaron dos features, pueden graficar las fronteras de decisión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mZXUi0zJvr4"
      },
      "source": [
        "N = 20 #para no graficar todos los puntos y saturar el grafico\n",
        "\n",
        "plt.figure(figsize = (8,6))\n",
        "\n",
        "#Grafico Clasificador Sesgado\n",
        "ax = sns.scatterplot(X_test[::N].MaxTemp, X_test[::N].Humidity3pm, hue=y_test[::N], palette='Set2')\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
        "                      np.linspace(*ylim, num=200))\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "contours = ax.contourf(xx, yy, Z, alpha=0.3, cmap = 'Set2')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQNvu9PlJvr7"
      },
      "source": [
        "## 3. Random Forest\n",
        "\n",
        "Random Forest, además de aplicar Bagging, también selecciona features al azar, de esa manera descorrelaciona aún más los distintos modelos de árbol creados.\n",
        "\n",
        "1. Importar de scikit-learn el modelo `RandomForestClassifier`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9XxMxz1Jvr8"
      },
      "source": [
        "from sklearn.COMPLETAR import COMPLETAR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6k1unuvJvsA"
      },
      "source": [
        "2. Investigar sus parámetros. En particular, `n_estimators`, `max_features` y `oob_score`. Luego, crear y entrenar un modelo en el conjunto de train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPKh6si1JvsB"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=COMPLETAR, max_features=COMPLETAR, n_jobs=-1, oob_score = True, random_state = 42)\n",
        "clf.fit(X_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMLLIS0FJvsF"
      },
      "source": [
        "3. Evaluar su desempeño en el conjunto de train y de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nw18y6MJvsG"
      },
      "source": [
        "y_train_pred = clf.COMPLETAR(COMPLETAR)\n",
        "y_test_pred = clf.COMPLETAR(COMPLETAR)\n",
        "print(metrics.COMPLETAR(COMPLETAR, COMPLETAR))\n",
        "print(metrics.COMPLETAR(COMPLETAR, COMPLETAR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lM5LEmxJvsJ"
      },
      "source": [
        "4. ¿Cuál es su `oob_score_`?¿Y que son `feature_importances_`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AbWPlIKJvsJ"
      },
      "source": [
        "clf.oob_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGgD2Xs3JvsL"
      },
      "source": [
        "clf.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "014iW4IYJvsO"
      },
      "source": [
        "# CORRER ESTA CELDA UNA VEZ QUE HAYAN ESTUDIADO QUE ES OOB_SCORE Y FEATURE_IMPORTANCES\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "columns = X_train.columns\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize = (15,8))\n",
        "sns.barplot(columns[indices], importances[indices])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI6jT96DJvsR"
      },
      "source": [
        "5. ¿Qué hay en la propiedad `estimators_`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY0zWZw6JvsR"
      },
      "source": [
        "clf.estimators_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGv026tLJvsU"
      },
      "source": [
        "6. Elegir uno de los `estimators` y evaluar su desempeño sobre train y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_A-3QURJvsU"
      },
      "source": [
        "clf_tree = clf.estimators_[COMPLETAR]\n",
        "clf_tree.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPSJnBHwJvsW"
      },
      "source": [
        "y_train_pred = clf_tree.predict(X_train)\n",
        "y_test_pred = clf_tree.predict(X_test)\n",
        "print(metrics.accuracy_score(y_train, y_train_pred))\n",
        "print(metrics.accuracy_score(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjJIp9WJvsZ"
      },
      "source": [
        "¿Está overfiteado?¿Por qué la accuracy sobre el conjunto de train no es 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf49sPGhJvsZ"
      },
      "source": [
        "7. Hacer y graficar la curva de validación/complejidad para un modelo Random Forest en función del número de estimadores. No usamos CV porque puede llevar bastante tiempo. Si quieren, lo pueden probar después. Además, obtener su oob_score para graficar en la curva de complejidad (No se preocupen por los mensajes de warning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSUel6JGJvsa"
      },
      "source": [
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "oob_scores = []\n",
        "\n",
        "N_estimadores = [1,2,3,4,5,10,25,50,100,250,500,1000]\n",
        "for estimadores in COMPLETAR:\n",
        "    print(estimadores)\n",
        "    clf = RandomForestClassifier(n_estimators=COMPLETAR, n_jobs=-1, oob_score= True, random_state = 42)\n",
        "    clf.fit(COMPLETAR,COMPLETAR)\n",
        "    \n",
        "    y_train_pred = clf.predict(COMPLETAR)\n",
        "    y_test_pred = clf.predict(COMPLETAR)\n",
        "    \n",
        "    train_accuracy.append(COMPLETAR)\n",
        "    test_accuracy.append(COMPLETAR)\n",
        "    oob_scores.append(clf.COMPLETAR)\n",
        "    \n",
        "train_accuracy = np.array(train_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "oob_scores = np.array(oob_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_3U1Pw6Jvsd"
      },
      "source": [
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(N_estimadores, COMPLETAR, label = 'Train')\n",
        "plt.plot(N_estimadores, COMPLETAR, label = 'Test')\n",
        "plt.plot(N_estimadores, COMPLETAR, label = 'OOB')\n",
        "plt.xlabel('Numero de estimadores')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "# plt.xlim(0,50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozuvMLtJJvsg"
      },
      "source": [
        "8. Hacer y graficar la curva de aprendizaje para un modelo con 250 estimadores. Puede llevar bastante tiempo, no se preocupen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6V7waywJvsh"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=250, n_jobs=-1, oob_score= True, random_state = 42)\n",
        "\n",
        "train_sizes, train_scores, valid_scores = learning_curve(COMPLETAR, COMPLETAR, COMPLETAR, \n",
        "                                                         train_sizes = np.linspace(0.0001,1,10),\n",
        "                                                         scoring = 'accuracy', cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Siv3xPKJvsk"
      },
      "source": [
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(COMPLETAR, COMPLETAR.mean(axis = COMPLETAR), color = 'r')\n",
        "plt.plot(COMPLETAR, COMPLETAR.mean(axis = COMPLETAR), color = 'g')\n",
        "\n",
        "plt.fill_between(COMPLETAR, COMPLETAR,\n",
        "                     COMPLETAR, alpha=0.25,\n",
        "                     color=\"r\")\n",
        "plt.fill_between(COMPLETAR, COMPLETAR,\n",
        "                     COMPLETAR, alpha=0.25, color=\"g\")\n",
        "\n",
        "plt.ylim(0.5,1.1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_QYSwq4Jvsm"
      },
      "source": [
        "9. Si usaron dos features, pueden graficar las fronteras de decisión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xunphIRtJvsn"
      },
      "source": [
        "N = 20 #para no graficar todos los puntos y saturar el grafico\n",
        "clf = RandomForestClassifier(n_estimators=250).fit(X_train, y_train)\n",
        "\n",
        "#COMPLETAR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-pDtsL9Jvso"
      },
      "source": [
        "**Ejercicio**: elegir más features y volver a entrenar.\n",
        "\n",
        "**Para pensar**: ¿qué otras métricas utilizarían para evaluar estos modelos, dadas las características particulares del problema? Comparar con los casos *benchmark* que hicieron."
      ]
    }
  ]
}